{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pyro.contrib.funsor`: a new backend for Pyro (pt. 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Part 1 of this tutorial, we looked at new low-level functionality, especially the new primitives `pyro.to_funsor` and `pyro.to_data`, and in Part 2 we saw a simple example of how those low-level components could be used to implement a powerful inference algorithm.\n",
    "\n",
    "In this, the final part of the `pyro.contrib.funsor` tutorial, we'll look at how the new implementations of higher-level Pyro machinery can be used together with Funsor to drastically simplify the implementation of Pyro's most powerful general-purpose inference engine, `pyro.infer.TraceEnum_ELBO`. What we'll end up with is not a toy: it's a fully functional version of `TraceEnum_ELBO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import funsor\n",
    "\n",
    "from pyro import set_rng_seed as pyro_set_rng_seed\n",
    "from pyro.infer import ELBO\n",
    "from pyro.infer import TraceEnum_ELBO as OrigTraceEnum_ELBO\n",
    "\n",
    "funsor.set_backend(\"torch\")\n",
    "torch.set_default_tensor_type(torch.float32)\n",
    "pyro_set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we'll make use of `pyroapi` to use `pyro.contrib.funsor` with existing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.funsor\n",
    "import pyroapi\n",
    "from pyroapi import handlers, infer, ops, optim, pyro\n",
    "from pyroapi import distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by reviewing what's actually being computed. Readers who need a refresher on the basics of variational inference in Pyro should check out the variational inference tutorials.\n",
    "\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to dive into the actual computation. We'll start from the topmost level (defining the `TraceEnum_ELBO` class) and work our way through each subroutine of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceEnum_ELBO(ELBO):\n",
    "\n",
    "    def _get_trace(self, *args, **kwargs):  # must be defined to avoid NotImplementedError\n",
    "        raise ValueError(\"shouldn't be here\")\n",
    "\n",
    "    @pyroapi.pyro_backend(\"contrib.funsor\")\n",
    "    def differentiable_loss(self, model, guide, *args, **kwargs):\n",
    "        \n",
    "        # get traces: this part is exactly the same in Pyro and the new backend\n",
    "        model_tr, guide_tr = get_traces(model, guide, -self.max_plate_nesting-1, *args, **kwargs)\n",
    "\n",
    "        # extract terms from the model and guide traces\n",
    "        model_terms, guide_terms = accumulate_terms(model_tr, guide_tr)\n",
    "\n",
    "        # contract out auxiliary variables in the model\n",
    "        model_costs = integrate_model_vars(model_terms[\"log_measures\"], model_terms[\"log_factors\"],\n",
    "                                           model_terms[\"measure_vars\"] - guide_terms[\"measure_vars\"],\n",
    "                                           model_terms[\"plate_vars\"] | guide_terms[\"plate_vars\"])\n",
    "        \n",
    "        # compute guide costs (-log(q) terms)\n",
    "        guide_costs = [-log_q for log_q in guide_terms[\"log_factors\"]]\n",
    "\n",
    "        # integrate out guide variables\n",
    "        elbo = integrate_guide_vars(guide_terms[\"log_measures\"], model_costs + guide_costs,\n",
    "                                    guide_terms[\"measure_vars\"] - model_terms[\"measure_vars\"],\n",
    "                                    model_terms[\"plate_vars\"] | guide_terms[\"plate_vars\"])\n",
    "\n",
    "        assert not elbo.inputs\n",
    "        with funsor.memoize.memoize():\n",
    "            # elbo is a lazy expression that we rewrite to an optimized form\n",
    "            return -pyro.to_data(funsor.optimizer.apply_optimizer(elbo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go through the four helper functions one by one, starting with `get_traces`. This is essentially identical to the code in the original `TraceEnum_ELBO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces(model, guide, first_available_dim, *args, **kwargs):\n",
    "    with handlers.enum(first_available_dim=first_available_dim):\n",
    "        guide_tr = handlers.trace(guide).get_trace(*args, **kwargs)\n",
    "        model_tr = handlers.trace(handlers.replay(model, trace=guide_tr)).get_trace(*args, **kwargs)\n",
    "    return model_tr, guide_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `accumulate_terms` helper next. This function extracts all of the necessary tensors to compute the ELBO from the model and guide traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_terms(model_tr, guide_tr):\n",
    "    model_terms = {\"log_factors\": [], \"log_measures\": [], \"plate_vars\": frozenset(), \"measure_vars\": frozenset()}\n",
    "    guide_terms = {\"log_factors\": [], \"log_measures\": [], \"plate_vars\": frozenset(), \"measure_vars\": frozenset()}\n",
    "    for terms, tr in zip((model_terms, guide_terms), (model_tr, guide_tr)):\n",
    "        for name, node in tr.nodes.items():\n",
    "            if node[\"type\"] != \"sample\" or type(node[\"fn\"]).__name__ == \"_Subsample\":\n",
    "                continue\n",
    "            # if a site is enumerated in the model, measure but no log_prob\n",
    "            if name in guide_tr.nodes or node['is_observed']:\n",
    "                terms[\"log_factors\"].append(node[\"funsor\"][\"log_prob\"])\n",
    "            if node[\"funsor\"].get(\"log_measure\", None) is not None:\n",
    "                terms[\"log_measures\"].append(node[\"funsor\"][\"log_measure\"])\n",
    "                terms[\"measure_vars\"] |= frozenset(node[\"funsor\"][\"log_measure\"].inputs)\n",
    "            terms[\"plate_vars\"] |= frozenset(f.name for f in node[\"cond_indep_stack\"] if f.vectorized)\n",
    "            terms[\"measure_vars\"] |= frozenset(node[\"funsor\"][\"log_prob\"].inputs)\n",
    "    return model_terms, guide_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `model_terms` and `guide_terms` data structures in hand, we're ready to integrate out the variables enumerated within the model. The function we'll use, `integrate_model_vars`, works entirely on `funsor.Funsor`s: it takes in lists of `funsor.Funsor`s and `frozenset`s of input names, and returns another list of modified `funsor.Funsor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@funsor.interpreteter.interpretation(funsor.terms.lazy)\n",
    "def integrate_model_vars(log_measures, log_factors, measure_vars, plate_vars):\n",
    "    contracted_terms = [t for t in log_factors if measure_vars & frozenset(t.inputs)]\n",
    "    uncontracted_terms = [t for t in log_factors if not measure_vars & frozenset(t.inputs)]\n",
    "    return uncontracted_terms + funsor.sum_product.partial_sum_product(\n",
    "        funsor.ops.logaddexp, funsor.ops.add,\n",
    "        log_measures + contracted_terms,\n",
    "        plates=plate_vars, eliminate=measure_vars - plate_vars\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `funsor.sum_product.partial_sum_product` in `integrate_model_vars`. This is an implementation of the tensor variable elimination algorithm that matches the description in the original paper nearly line-by-line.\n",
    "\n",
    "After integrating out variables that were enumerated in the model, all that's left are variables that were enumerated in the guide. The `integrate_guide_vars` function that we'll use to compute the final ELBO also operates entirely on `funsor.Funsor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@funsor.interpreter.interpretation(funsor.terms.lazy)\n",
    "def integrate_guide_vars(log_measures, costs, measure_vars, plate_vars):\n",
    "    result = funsor.to_funsor(0, output=funsor.reals())\n",
    "    with funsor.memoize.memoize():\n",
    "        for cost in costs:\n",
    "            log_measure = funsor.sum_product.sum_product(\n",
    "                funsor.ops.logaddexp, funsor.ops.add, log_measures,\n",
    "                plates=plate_vars, eliminate=(plate_vars | measure_vars) - frozenset(cost.inputs)\n",
    "            )\n",
    "            term = funsor.Integrate(log_measure, cost, measure_vars & frozenset(cost.inputs))\n",
    "            term = term.reduce(funsor.ops.add, plate_vars & frozenset(cost.inputs))\n",
    "            result = result + term\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `integrate_model_vars`, `integrate_guide_vars` relies heavily on `funsor.sum_product`. However, unlike `integrate_model_vars`, the integral over the guide variables has moved outside the logarithm. This means that the ELBO decomposes into a sum of expectations of individual ELBO terms wrt the full guide distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
